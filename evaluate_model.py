"""
Evaluate the trained lead scoring model on a held-out test set.

Usage:
    python3 evaluate_model.py
"""

import pickle

import pandas as pd
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split


def main() -> None:
    # Load trained artifacts
    with open("lead_scoring_model_xgb.pkl", "rb") as f:
        artifacts = pickle.load(f)

    model = artifacts["model"]
    scaler = artifacts["scaler"]
    label_encoder = artifacts["label_encoder"]

    # Load full dataset generated by train_model.py
    df = pd.read_csv("lead_scoring_dataset.csv")

    X = df.drop("lead_score", axis=1)
    y = df["lead_score"]

    # Encode labels using the same encoder used during training
    y_encoded = label_encoder.transform(y)

    # Same split config as training for a fair evaluation
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y_encoded,
        test_size=0.2,
        random_state=42,
        stratify=y_encoded,
    )

    # Scale with the previously fitted scaler
    X_test_scaled = scaler.transform(X_test)

    # Predict
    y_pred = model.predict(X_test_scaled)

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    report = classification_report(
        y_test, y_pred, target_names=label_encoder.classes_
    )
    cm = confusion_matrix(y_test, y_pred)

    print("\n=== Lead Scoring Model Evaluation ===")
    print(f"Accuracy: {acc:.4f} ({acc*100:.2f}%)\n")
    print("Classification report:")
    print(report)
    print("Confusion matrix (rows=true, cols=predicted):")
    print(cm)


if __name__ == "__main__":
    main()


